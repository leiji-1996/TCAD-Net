#!/usr/bin/env python3
# -*- coding: utf-8 -*-
from joblib import Memory


mem = Memory("./dataset/svm_data")

@mem.cache
def get_data_from_svmlight_file(path):
    data = load_svmlight_file(path)
    return data[0], data[1]

def dataLoading(path):
    # loading data
    df = pd.read_csv(path) 
    
    labels = df['class']
    
    x_df = df.drop(['class'], axis=1)
    
    x = x_df.values
    print("Data shape: (%d, %d)" % x.shape)
    
    return x, labels;


def aucPerformance(mse, labels):
    roc_auc = roc_auc_score(labels, mse)
    ap = average_precision_score(labels, mse)
    print("AUC-ROC: %.4f, AUC-PR: %.4f" % (roc_auc, ap))
    return roc_auc, ap;

def writeResults(name, n_samples, dim, n_samples_trn, n_outliers_trn, n_outliers, depth, rauc, ap, std_auc, std_ap, train_time, test_time, path = "./results/auc_performance_cl0.5.csv"):    
    csv_file = open(path, 'a') 
    row = name + "," + str(n_samples)+ ","  + str(dim) + ',' + str(n_samples_trn) + ','+ str(n_outliers_trn) + ','+ str(n_outliers)  + ',' + str(depth)+ "," + str(rauc) +"," + str(std_auc) + "," + str(ap) +"," + str(std_ap)+"," + str(train_time)+"," + str(test_time) + "\n"
    csv_file.write(row)

#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
@author: Guansong Pang
The algorithm was implemented using Python 3.6.6, Keras 2.2.2 and TensorFlow 1.10.1.
More details can be found in our KDD19 paper.
Guansong Pang, Chunhua Shen, and Anton van den Hengel. 2019. 
Deep Anomaly Detection with Deviation Networks. 
In The 25th ACM SIGKDDConference on Knowledge Discovery and Data Mining (KDD ’19),
August4–8, 2019, Anchorage, AK, USA.ACM, New York, NY, USA, 10 pages. https://doi.org/10.1145/3292500.3330871
"""

import pandas as pd
import numpy as np
from sklearn.metrics import auc,roc_curve, precision_recall_curve, average_precision_score, roc_auc_score
from sklearn import preprocessing
import matplotlib.pyplot as plt
# from sklearn.externals.joblib import Memory
from joblib import Memory
from sklearn.datasets import load_svmlight_file

mem = Memory("./dataset/svm_data")

@mem.cache
def get_data_from_svmlight_file(path):
    #将 svmlight /libsvm 格式的数据集加载到稀疏 CSR 矩阵中
    data = load_svmlight_file(path)
    return data[0], data[1]

def dataLoading(path):
    # loading data
    df = pd.read_csv(path) 
    #按class标签进行分类
    labels = df['class']
    x_df = df.drop(['class'], axis=1)
    x = x_df.values

    # labels = df.iloc[:, -1:]
    # x_df = df.iloc[:, :-1]
    print("Data shape: (%d, %d)" % x.shape)
    
    return x, labels;

def dataLoading_vaild(path):
    # loading data
    df = pd.read_csv(path)
    feature = df.columns.tolist()
    # 按class标签进行分类
    labels = df['class']
    x_df = df.drop(['class'], axis=1)
    x = x_df.values

    # labels = df.iloc[:, -1:]
    # x_df = df.iloc[:, :-1]
    print("Data shape: (%d, %d)" % x.shape)

    return x, labels, feature;


def aucPerformance(mse, labels):
    # 计算AUC-ROC and AUC-PR
    # y_true：样本的真实标签，形状（样本数，）
    # y_score：预测为1的概率值，形状（样本数，）
    roc_auc = roc_auc_score(labels, mse)
    ap = average_precision_score(labels, mse)
    print("AUC-ROC: %.4f, AUC-PR: %.4f" % (roc_auc, ap))
    return roc_auc, ap

def writeResults(name, n_samples, dim, n_samples_trn, n_outliers_trn, n_outliers, depth, rauc, ap, std_auc, std_ap, train_time, test_time, path = "./results/auc_performance_cl0.5.csv"):    
    csv_file = open(path, 'a') 
    row = name + "," + str(n_samples)+ ","  + str(dim) + ',' + str(n_samples_trn) + ','+ str(n_outliers_trn) + ','+ str(n_outliers)  + ',' + str(depth)+ "," + str(rauc) +"," + str(std_auc) + "," + str(ap) +"," + str(std_ap)+"," + str(train_time)+"," + str(test_time) + "\n"
    csv_file.write(row)

def inject_noise(seed, n_out, random_seed):
    '''
    add anomalies to training data to replicate anomaly contaminated data sets.
    we randomly swape 5% features of anomalies to avoid duplicate contaminated anomalies.
    this is for dense data
    向训练数据中添加异常，复制异常的数据集，并添加噪声。我们随机交换5%的异常特征，以避免重复污染异常。这是针对密集数据的
    parameter：
        seed：污染数据
        n_out：插入个数
        random_seed：随机数种子
    '''
    rng = np.random.RandomState(random_seed)
    n_sample, dim = seed.shape
    swap_ratio = 0.05 # 交换率
    n_swap_feat = int(swap_ratio * dim)
    noise = np.empty((n_out, dim))
    for i in np.arange(n_out):
        outlier_idx = rng.choice(n_sample, 2, replace = False)
        o1 = seed[outlier_idx[0]]
        o2 = seed[outlier_idx[1]]
        # 交换的列，列数由n_swap_feat决定
        swap_feats = rng.choice(dim, n_swap_feat, replace = False)
        noise[i] = o1.copy()
        # 引入swap_feats个噪声从o2中获取
        noise[i, swap_feats] = o2[swap_feats]
    return noise
